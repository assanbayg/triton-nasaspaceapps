#import libraries
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import statsmodels.formula.api as smf
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.model_selection import train_test_split

%matplotlib inline

#set printing options
np.set_printoptions(precision=3, suppress=True)


#set column names and read our csv file
column_names = ['Temperature', 'RH', 'Ws','Rain','FFMC','DMC','DC','ISI','BUI','FWI']
df= pd.read_csv('/content/fire_weather2 (2).csv', names=column_names,
                          na_values='?', comment='\t',
                          sep=',', skipinitialspace=True)
df= df.drop(0)
df.reset_index(drop=True, inplace=True)
df.head()

#let us get rid of non-numeric values
df.isna().sum()
data = df.dropna()

data.head() #let's check the head of our dataframe

data.dtypes #let's check columns data types

#if we have non-float column, this lines of code will translate all columns to the float datatype
import pandas as pd
columns_list=['Temperature', 'RH', 'Ws','Rain','FFMC','DMC','DC','ISI','BUI','FWI',]
for i in columns_list:
  data[i] = pd.to_numeric(data[i], errors='coerce')
  non_numeric_rows = data[data[i].isna()]
  data = data.dropna(subset=[i])
  data[i] = data[i].fillna(0)
  data[i] = data[i].astype(float)

data.dtypes

#dividing out dataset into training and test dataset
train_dataset = data.sample(frac=0.8, random_state=0)
test_dataset = data.drop(train_dataset.index)

#let's choose our independent and dependent values
train_features = train_dataset.copy()
test_features = test_dataset.copy()

train_labels = train_features.pop('FWI')
test_labels = test_features.pop('FWI')

train_features.corr() #checking correlation between datas

#checking for the multicollinearity
plt.figure(figsize=(12,10))
corr=train_features.corr()
sns.heatmap(corr,annot=True)

def correlation(dataset, threshold):
    col_corr = set()
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                colname = corr_matrix.columns[i]
                col_corr.add(colname)
    return col_corr

corr_features=correlation(train_features,0.85) #Thresholding, let's check if we have datas that correlated with each other very strong

NOTE FOR PREVIOUS CODE: IF datas correlatd with each other extremely strong, it will lead to decrease of accuracy of our model, because they could redundant in our code

corr_features #checking if we have strong correlated datas

print(train_features.columns)

#if we have that kind of datas we will remove one of the column from these features in order to avoid multicollinearuity
train_features.drop(columns = corr_features, axis = 1, inplace=True)
test_features.drop(columns = corr_features,axis = 1, inplace=True)
train_features.shape
test_features.shape

#Feature scaling and standartization
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
train_features_scaled=scaler.fit_transform(train_features)
test_features_scaled=scaler.transform(test_features)

train_features_scaled

plt.subplots(figsize=(15, 5))
plt.subplot(1, 2, 1)
sns.boxplot(data=train_features)
plt.title('Train features Before Scaling')
plt.subplot(1, 2, 2)
sns.boxplot(data=train_features_scaled)
plt.title('Train features After Scaling')

#checking on which values our label is dependent
lm1 = smf.ols(formula='FWI ~ RH + Temperature + Ws + DC + ISI + BUI + FFMC + Rain + DMC', data=data).fit()
lm1.params

#Finding coefficients
feature_cols = ['RH', 'Rain', 'Temperature', 'Ws', 'DC', 'ISI', 'BUI', 'FFMC', 'DMC']
X = data[feature_cols]
y = data['FWI']


lm2 = LinearRegression()
lm2.fit(X, y)


print(lm2.intercept_)
print(lm2.coef_)

list(zip(feature_cols, lm2.coef_)) #let's pair feature names with coefficients

lm1.summary() #let's see the overall information

#Let's check for MSE if test size is 0.25 from all information
X = data[['RH', 'Rain', 'Temperature', 'Ws', 'DC', 'ISI', 'BUI', 'FFMC', 'DMC']]
y = data['FWI']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)


lm2 = LinearRegression()


lm2.fit(X_train, y_train)


y_pred = lm2.predict(X_test)


print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))


#Building our model and looking prediction and true values
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
linreg=LinearRegression()
linreg.fit(train_features_scaled,train_labels)
y_pred=linreg.predict(test_features_scaled)
mae=mean_absolute_error(test_labels,y_pred)
score=r2_score(test_labels,y_pred)
print("Mean absolute error: ", mae)
print("R2 Score: ", score)
print("Prediction: ", y_pred)
print("Test labels: ", test_labels)

for i in y_pred:
  print((i*100)/data['FWI'].max())

